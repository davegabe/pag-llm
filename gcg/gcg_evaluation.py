from dataclasses import dataclass
from typing import Any

import torch
from tqdm import tqdm

from data.data_processor import TextDataset
from gcg import gcg_algorithm, gcg_utils


@dataclass
class GCGResult:
    """
    Class to store the results of a GCG attack, for later analysis.
    """

    """
    Original prefix IDs, which are the first few tokens of the input text.
    """
    original_prefix_ids: list[int]

    """
    Target response IDs, which are the tokens we want to have as a completion.
    """
    target_response_ids: list[int]

    """
    Attack input string, which is the string generated by the GCG algorithm.
    """
    x_attack_str: str

    """
    Attack response IDs, which are the tokens generated by the LLM given the attack input string.
    """
    y_attack_response_ids: list[int]

    """
    Number of steps taken to generate the attack input string.
    """
    steps: int

    def to_dict(self) -> dict[str, Any]:
        """
        Convert the GCGResult object to a dictionary for easy serialization.
        """
        return {
            'original_prefix_ids': self.original_prefix_ids,
            'target_response_ids': self.target_response_ids,
            'x_attack_str': self.x_attack_str,
            'y_attack_response_ids': self.y_attack_response_ids,
            'steps': self.steps,
        }

    @staticmethod
    def from_dict(data: dict[str, Any]) -> 'GCGResult':
        """
        Create a GCGResult object from a dictionary.
        """
        return GCGResult(
            original_prefix_ids=data['original_prefix_ids'],
            target_response_ids=data['target_response_ids'],
            x_attack_str=data['x_attack_str'],
            y_attack_response_ids=data['y_attack_response_ids'],
            steps=data['steps'],
        )


def evaluate_model_with_gcg(gcg: gcg_algorithm.GCG,
                            dataset: TextDataset,
                            target_response_len: int,
                            random_select_samples: bool = True,
                            max_samples_to_attack: int | None = None) -> list[GCGResult]:
    """
        Run GCG evaluation on the dataset.

        Args:
            gcg: GCG object
            dataset: Dataset to evaluate on
            target_response_len: Length of the target attack response we want to generate
            random_select_samples: Whether to randomly select samples from the dataset or not
            max_samples_to_attack: Maximum number of samples to attack. If None, all samples will be attacked.

        Returns:
            attack_success: Number of successful attacks
            attacks_total: Total number of attacks attempted
            steps_run: Total number of steps run for the successful attacks
        """
    gcg_utils.set_seeds()
    prefix_len = gcg.num_prefix_tokens

    max_samples_to_attack = len(dataset) if max_samples_to_attack is None else max_samples_to_attack
    max_samples_to_attack = min(max_samples_to_attack, len(dataset))

    samples_to_attack = torch.randperm(len(dataset))[:max_samples_to_attack].tolist() \
        if random_select_samples \
        else list(range(max_samples_to_attack))

    attacks: list[GCGResult] = []

    # Iterate through the dataset and attack each example
    for sample_idx in tqdm(samples_to_attack, desc="Evaluating with GCG"):
        item = dataset[sample_idx]
        input_ids = item.input_ids[item.attention_mask == 1]
        if input_ids.size(-1) <= prefix_len + target_response_len:
            # Skip if the target response is too short
            continue

        # Split into prefix and target response
        original_prefix_ids = input_ids[:prefix_len]
        # original_prefix_str = gcg.tokenizer.decode(original_prefix_ids)
        target_response_ids = input_ids[prefix_len:prefix_len + target_response_len]
        target_response_str = gcg.tokenizer.decode(target_response_ids)

        # Run the attack
        x_attack_str, y_attack_response, steps = gcg.run(target_response_str, show_progress=False)
        # noinspection PyTypeChecker
        y_attack_response_ids: torch.Tensor = gcg.tokenizer.encode(y_attack_response, return_tensors='pt')[0]

        attacks.append(GCGResult(
            original_prefix_ids=original_prefix_ids.detach().cpu().tolist(),
            target_response_ids=target_response_ids.detach().cpu().tolist(),
            x_attack_str=x_attack_str,
            y_attack_response_ids=y_attack_response_ids.detach().cpu().tolist(),
            steps=steps,
        ))

    return attacks
