_target_: config.CustomLLMPagConfig

training:
  _target_: config.TrainingConfig
  learning_rate: 0.004
  batch_size: 256
  gradient_accumulation_steps: 8
  warmup_pretrain_epochs: 0
  num_epochs: 5
  warmup_steps: 100
  weight_decay: 0.01
  max_seq_length: 256
  method: 'pag-identity-embeddings'
  pag_classes: 256
  lora:
    _target_: config.LoraTConfig
  # Hyperparameters to give different importance to parts of the loss function
  lambda_loss_ce: 1.0
  lambda_loss_pag: 0.5
  run_evaluation_before_training: False

model:
  _target_: config.CustomModelConfig
  hidden_layer_index: 5
  hidden_size: 256
  intermediate_size: 640
  num_attention_heads: 16
  num_hidden_layers: 3
  num_key_value_heads: 4
  tie_word_embeddings: True
  vocab_size: 2048 # This will be automatically updated to match the tokenizer
  max_position_embeddings: 256
  output_dir: '${path:checkpoints/tinystories-pretokenized}'

dataset:
  _target_: config.DatasetConfig
  name: 'fhswf/TinyStoriesV2_cleaned' # not used when use_pretokenized is True
  
  # Pre-tokenized dataset configuration
  use_pretokenized: True
  local_dataset_path: 'offline_assets/datasets/DaveGabe_TinyStoriesV2_cleaned-voc2048-seq256-overlap25'
  local_tokenizer_path: 'offline_assets/tokenizers/DaveGabe_TinyStoriesV2_cleaned-voc2048-seq256-overlap25-bpe-tokenizer'
  # pretokenized_dataset_name: 'DaveGabe/TinyStoriesV2_cleaned-voc2048-seq256-overlap25'
  # pretrained_tokenizer_name: 'DaveGabe/TinyStoriesV2_cleaned-voc2048-seq256-overlap25-bpe-tokenizer'

  train_split: 'train'
  eval_split: 'validation'
  test_split: 'test'
  num_workers: 6
  prefix:
    _target_: config.DatasetPrefixConfig
    min_length: 10
    max_length: 33    # It has been observed that all samples have >= 34 tokens
    dataset_index_path: '${path:checkpoints/finetuned-smollm2/dataset_index_by_token.pt}'

logging:
  _target_: config.LoggingConfig
  logging_steps: 20
  evaluation_steps: 200
  save_steps: 1000