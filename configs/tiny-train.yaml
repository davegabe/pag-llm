_target_: config.LLMPagConfig

training:
  _target_: config.TrainingConfig
  learning_rate: 0.004629403549377777
  batch_size: 16
  gradient_accumulation_steps: 1
  num_epochs: 2
  warmup_steps: 100
  weight_decay: 0.01
  max_seq_length: 4096
  method: 'base' # 'base', 'pag-hidden'
  pag_classes: 256
  lora:
    _target_: config.LoraTConfig
  # Hyperparameters to give different importance to parts of the loss function
  lambda_loss_ce: 1.0
  lambda_loss_pag: 2.0

model:
  _target_: config.ModelConfig
  pretrained_base: 'raincandy-u/TinyStories-656K'
  random_initialization: True
  output_dir: '${path:checkpoints/tinystories}'
  hidden_layer_index: 2

dataset:
  _target_: config.DatasetConfig
  name: 'raincandy-u/TinyStoriesV2_SpecialTokens'

  train_split: 'train'
  eval_split: 'validation'
  num_workers: 3
  prefix:
    _target_: config.DatasetPrefixConfig
    min_length: 10
    max_length: 33    # It has been observed that all samples have >= 34 tokens
    dataset_index_path: '${path:checkpoints/finetuned-smollm2/dataset_index_by_token.pt}'

logging:
  _target_: config.LoggingConfig
  logging_steps: 20
  evaluation_steps: 2000
  save_steps: 2000