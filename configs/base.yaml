_target_: config.Config

training:
  _target_: config.TrainingConfig
  learning_rate: 5e-5
  batch_size: 4
  gradient_accumulation_steps: 32
  num_epochs: 3
  warmup_steps: 100
  weight_decay: 0.01
  max_seq_length: 512
  method: 'pag-hidden' # 'base', 'pag-hidden'
  pag_classes: 8
  lora:
    _target_: config.LoraTConfig
    use_lora: True
    lora_rank: 8
    lora_dropout: 0.1

model:
  _target_: config.ModelConfig
  pretrained_base: 'HuggingFaceTB/SmolLM2-135M'
  output_dir: '${path:checkpoints/finetuned-smollm2}'
  hidden_layer_index: 20

dataset:
  _target_: config.DatasetConfig

#  name: 'HuggingFaceFW/fineweb-edu'
#  config: 'CC-MAIN-2016-26'

  name: 'parquet'   # File format for the data_files
  files_to_download:
    # TRAIN split
    - 'https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/main/data/CC-MAIN-2016-26/train-00000-of-00010.parquet'
    - 'https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/main/data/CC-MAIN-2016-26/train-00001-of-00010.parquet'
    #    - 'https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/main/data/CC-MAIN-2016-26/train-00002-of-00010.parquet'
    # VALIDATION split
    - 'https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/resolve/main/data/CC-MAIN-2016-26/train-00003-of-00010.parquet'
  data_files:
    train:
      - 'downloaded_dataset/train-00000-of-00010.parquet'
      - 'downloaded_dataset/train-00001-of-00010.parquet'
    #      - 'downloaded_dataset/train-00002-of-00010.parquet'
    validation:
      - 'downloaded_dataset/train-00003-of-00010.parquet'

  train_split: 'train'
  eval_split: 'validation'
  num_workers: 3
  prefix:
    _target_: config.DatasetPrefixConfig
    min_length: 10
    max_length: 33    # It has been observed that all samples have >= 34 tokens

logging:
  _target_: config.LoggingConfig
  logging_steps: 20
  evaluation_steps: 2000
  save_steps: 2000