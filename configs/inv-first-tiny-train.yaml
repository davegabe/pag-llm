_target_: config.CustomLLMPagConfig

training:
  _target_: config.TrainingConfig
  learning_rate: 0.004
  batch_size: 16
  gradient_accumulation_steps: 1
  warmup_pretrain_epochs: 0
  overfit: False
  num_epochs: 1000
  warmup_steps: 100
  weight_decay: 0.01
  max_seq_length: 256
  method: 'identity-grad'
  pag_classes: 256
  lora:
    _target_: config.LoraTConfig
  # Hyperparameters to give different importance to parts of the loss function
  lambda_loss_ce: 2
  lambda_loss_pag: 1
  run_evaluation_before_training: False

model:
  _target_: config.CustomModelConfig
  hidden_layer_index: 5
  hidden_size: 512
  intermediate_size: 1024
  num_attention_heads: 16
  num_hidden_layers: 8
  num_key_value_heads: 4
  tie_word_embeddings: True
  vocab_size: 2048
  max_position_embeddings: 256
  output_dir: '${path:checkpoints/wikitext}'

dataset:
  _target_: config.DatasetConfig
  name: 'DaveGabe/wikitext103-overlapping-bpe'
  config: null
  tokenizer_name: 'DaveGabe/wikitext103-overlapping-bpe-tokenizer'

  train_split: 'train'
  eval_split: 'test' # 'validation' is not available in the dataset
  num_workers: 3
  prefix:
    _target_: config.DatasetPrefixConfig
    min_length: 10
    max_length: 33    # It has been observed that all samples have >= 34 tokens
    dataset_index_path: '${path:checkpoints/finetuned-smollm2/dataset_index_by_token.pt}'

logging:
  _target_: config.LoggingConfig
  logging_steps: 20
  evaluation_steps: 2000
  save_steps: 2000