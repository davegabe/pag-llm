training:
  learning_rate: 5e-5
  batch_size: 4
  gradient_accumulation_steps: 1   # TODO: Check if this works correctly when > 1
  num_epochs: 3
  warmup_steps: 100
  weight_decay: 0.01
  max_seq_length: 512

model:
  pretrained_base: 'HuggingFaceTB/SmolLM2-135M'
  output_dir: '${path:checkpoints/finetuned-smollm2}'

dataset:
  name: 'wikitext'
  config: 'wikitext-2-raw-v1'
  train_split: 'train'
  eval_split: 'validation'

logging:
  logging_steps: 100
  evaluation_steps: 500
  save_steps: 1000