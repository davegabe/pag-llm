_target_: config.CustomLLMPagConfig

training:
  _target_: config.TrainingConfig
  learning_rate: 0.004
  batch_size: 128
  gradient_accumulation_steps: 1
  warmup_pretrain_epochs: 0
  num_epochs: 5
  warmup_steps: 100
  weight_decay: 0.01
  max_seq_length: 256
  method: 'inv-first'
  pag_classes: 256
  lora:
    _target_: config.LoraTConfig
  # Hyperparameters to give different importance to parts of the loss function
  lambda_loss_ce: 1.0
  lambda_loss_pag: 2.0
  run_evaluation_before_training: True

model:
  _target_: config.CustomModelConfig
  hidden_layer_index: 2
  hidden_size: 128
  intermediate_size: 640
  num_attention_heads: 16
  num_hidden_layers: 3
  num_key_value_heads: 4
  tie_word_embeddings: True
  vocab_size: 2048
  max_position_embeddings: 256
  output_dir: '${path:checkpoints/tinystories}'

dataset:
  _target_: config.DatasetConfig
  name: 'fhswf/TinyStoriesV2_cleaned'

  train_split: 'train'
  eval_split: 'test' # 'validation' is not available in the dataset
  num_workers: 3
  prefix:
    _target_: config.DatasetPrefixConfig
    min_length: 10
    max_length: 33    # It has been observed that all samples have >= 34 tokens
    dataset_index_path: '${path:checkpoints/finetuned-smollm2/dataset_index_by_token.pt}'

logging:
  _target_: config.LoggingConfig
  logging_steps: 20
  evaluation_steps: 2000
  save_steps: 2000