from dataclasses import dataclass
from typing import Any
import torch
from tqdm import tqdm
from infer_backward_tinystories import compute_semantic_similarity, get_batch_perplexity
from models.base_model import BaseLMModel
from torch.nn.utils.rnn import pad_sequence
from sentence_transformers import SentenceTransformer

from data.data_processor import PreTokenizedDataset, TextDataset
from gcg import gcg_algorithm, gcg_utils


@dataclass
class GCGResult:
    """
    Class to store the results of a GCG attack, for later analysis.
    """

    """
    Original prefix IDs, which are the first few tokens of the input text.
    """
    original_prefix_ids: list[int]

    """
    Target response IDs, which are the tokens we want to have as a completion.
    """
    target_response_ids: list[int]

    """
    Attack input string, which is the string generated by the GCG algorithm.
    """
    x_attack_str: str

    """
    Attack response IDs, which are the tokens generated by the LLM given the attack input string.
    """
    y_attack_response_ids: list[int]

    """
    Number of steps taken to generate the attack input string.
    """
    steps: int

    def to_dict(self) -> dict[str, Any]:
        """
        Convert the GCGResult object to a dictionary for easy serialization.
        """
        return {
            'original_prefix_ids': self.original_prefix_ids,
            'target_response_ids': self.target_response_ids,
            'x_attack_str': self.x_attack_str,
            'y_attack_response_ids': self.y_attack_response_ids,
            'steps': self.steps,
        }

    @staticmethod
    def from_dict(data: dict[str, Any]) -> 'GCGResult':
        """
        Create a GCGResult object from a dictionary.
        """
        return GCGResult(
            original_prefix_ids=data['original_prefix_ids'],
            target_response_ids=data['target_response_ids'],
            x_attack_str=data['x_attack_str'],
            y_attack_response_ids=data['y_attack_response_ids'],
            steps=data['steps'],
        )

    def get_success_tokens(self) -> int:
        return sum(
            1
            for target_token, attack_response_token in zip(self.target_response_ids, self.y_attack_response_ids)
            if target_token == attack_response_token
        )
    

def evaluate_model_with_gcg(
        gcg: gcg_algorithm.GCG,
        dataset: TextDataset | PreTokenizedDataset,
        target_response_len: int,
        random_select_samples: bool = True,
        max_samples_to_attack: int | None = None
    ) -> list[GCGResult]:
    """
    Run GCG evaluation with intermediate convergence logging.
    This function runs GCG once per sample and logs metrics at specified intervals.
    
    Args:
        gcg: GCG object
        dataset: Dataset to evaluate on
        target_response_len: Length of the target attack response we want to generate
        random_select_samples: Whether to randomly select samples from the dataset or not
        max_samples_to_attack: Maximum number of samples to attack. If None, all samples will be attacked.
        evaluate_every_n_steps: Number of steps between metric evaluations and logging
        lightning_model: Language model for computing naturalness metrics
        semantic_model: SentenceTransformer model for semantic similarity
        
    Returns:
        List of GCGResult objects from the final evaluation
    """
    gcg_utils.set_seeds()
    prefix_len = gcg.num_prefix_tokens

    max_samples_to_attack = len(dataset) if max_samples_to_attack is None else max_samples_to_attack
    max_samples_to_attack = min(max_samples_to_attack, len(dataset))

    if max_samples_to_attack < len(dataset) and random_select_samples:
        samples_to_attack = torch.randperm(len(dataset))[:max_samples_to_attack].tolist()
    else:
        samples_to_attack = list(range(max_samples_to_attack))

    final_results: list[GCGResult] = []

    # Iterate through the dataset and attack each example (only final results are stored)
    for sample_idx in tqdm(samples_to_attack, desc="Running GCG Attacks"):
        item = dataset[sample_idx]
        input_ids = item.input_ids[item.attention_mask == 1]
        if input_ids.size(-1) <= prefix_len + target_response_len:
            # Skip if the target response is too short
            continue

        # Split into prefix and target response
        original_prefix_ids = input_ids[:prefix_len]
        target_response_ids = input_ids[prefix_len:prefix_len + target_response_len]
        target_response_str = gcg.tokenizer.decode(target_response_ids)
        # Run the attack (no intermediate snapshot capture)
        x_attack_str, y_attack_str, total_steps = gcg.run(
            target_response_str,
            show_progress=False,
            sample_idx=sample_idx
        )

        # Store the final attack for this sample
        y_final_ids = gcg.tokenizer.encode(y_attack_str, return_tensors="pt")[0]
        result = GCGResult(
            original_prefix_ids=original_prefix_ids.detach().cpu().tolist(),
            target_response_ids=target_response_ids.detach().cpu().tolist(),
            x_attack_str=x_attack_str,
            y_attack_response_ids=y_final_ids.detach().cpu().tolist(), # type: ignore
            steps=total_steps,
        )
        final_results.append(result)
    return final_results



@torch.no_grad()
def count_repeated_tokens(x_input_ids: torch.Tensor, x_attention_mask: torch.Tensor | None = None) -> torch.Tensor:
    """
    Count the number of repeated tokens in the given input IDs, considering the attention mask for valid tokens.
    Adapted from infer_backward_tinystories.py
    
    Args:
        x_input_ids: Input IDs of the sentences.
        x_attention_mask: Attention mask of the sentences. If None, assumes all tokens are valid.

    Returns:
        torch.Tensor: The number of repeated tokens for each sentence.
    """
    if x_input_ids.ndim == 1:
        x_input_ids = x_input_ids.unsqueeze(0)
    
    if x_attention_mask is None:
        x_attention_mask = torch.ones_like(x_input_ids)
    elif x_attention_mask.ndim == 1:
        x_attention_mask = x_attention_mask.unsqueeze(0)
    
    assert x_input_ids.shape == x_attention_mask.shape, \
        f'input_ids and attention_mask shape mismatch: {x_input_ids.shape} != {x_attention_mask.shape}'
    assert x_input_ids.ndim == 2, \
        f'input_ids shape mismatch: {x_input_ids.shape} != (batch_size, seq_len)'

    num_classes = x_input_ids.max().item() + 1

    # Set an invalid token ID where the attention_mask is zero
    invalid_token_id = int(num_classes)
    x_input_ids = x_input_ids.masked_fill(x_attention_mask == 0, invalid_token_id)

    # Get the counts of each token in the batch, keeping the batch dimension
    target = torch.zeros(x_input_ids.size(0), invalid_token_id + 1, dtype=x_input_ids.dtype, device=x_input_ids.device)
    values = torch.ones_like(x_input_ids)
    target.scatter_add_(dim=1, index=x_input_ids, src=values)

    # Remove the invalid token id from the target
    target = target[:, :invalid_token_id]

    # Now, remove zeros (tokens not showing in the input_ids) and ones (tokens not repeating) from the target
    target = target.masked_fill_(target < 2, 1)
    total_repeated_tokens = (target - 1).sum(dim=1)
    return total_repeated_tokens


def compute_non_alphanumeric_ratio(text: str) -> float:
    """
    Compute the ratio of non-alphanumeric characters in the text.
    
    Args:
        text: Input text string
        
    Returns:
        float: Ratio of non-alphanumeric characters (0.0 to 1.0)
    """
    if not text:
        return 0.0
    
    non_alphanumeric_count = sum(1 for char in text if not char.isalnum())
    return non_alphanumeric_count / len(text)


def compute_max_consecutive_token_repetition(input_ids: torch.Tensor, attention_mask: torch.Tensor | None = None) -> int:
    """
    Compute the maximum consecutive token repetition in the input sequence.
    
    Args:
        input_ids: Input token IDs (1D tensor)
        attention_mask: Attention mask (1D tensor). If None, assumes all tokens are valid.
        
    Returns:
        int: Maximum consecutive token repetition length
    """
    if input_ids.ndim > 1:
        input_ids = input_ids.squeeze()
    
    if attention_mask is None:
        attention_mask = torch.ones_like(input_ids)
    elif attention_mask.ndim > 1:
        attention_mask = attention_mask.squeeze()
    
    # Get valid tokens only
    valid_tokens = input_ids[attention_mask == 1]
    
    if len(valid_tokens) <= 1:
        return 1
    
    max_consecutive = 1
    current_consecutive = 1
    
    for i in range(1, len(valid_tokens)):
        if valid_tokens[i] == valid_tokens[i-1]:
            current_consecutive += 1
            max_consecutive = max(max_consecutive, current_consecutive)
        else:
            current_consecutive = 1
    
    return max_consecutive


def compute_naturalness_metrics(gcg_results: list[GCGResult], 
                               lightning_model: BaseLMModel,
                               semantic_model: SentenceTransformer,
                               step_interval: int,
                               batch_size: int = 128) -> dict:
    """
    Compute naturalness metrics for GCG-generated prefixes.
    
    Args:
        gcg_results: List of GCG results
        lightning_model: The language model for computing perplexity
        semantic_model: SentenceTransformer model for semantic similarity
        step_interval: The step interval for logging
        batch_size: Batch size for processing
        
    Returns:
        dict: Dictionary containing all computed metrics
    """
    if not gcg_results:
        return {}
    
    tokenizer = lightning_model.tokenizer
    device = lightning_model.device
    
    # Collect all metrics
    prefix_perplexities = []
    total_token_repetitions = []
    semantic_similarities = []
    non_alphanumeric_ratios = []
    max_consecutive_repetitions = []
    
    # Process in batches
    for start_i in range(0, len(gcg_results), batch_size):
        end_i = min(start_i + batch_size, len(gcg_results))
        batch = gcg_results[start_i:end_i]
        
        # Handle tokenizers that might not have a pad token
        pad_token_id: Any = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
        
        # Prepare batch data
        original_prefix_ids = pad_sequence([torch.tensor(result.original_prefix_ids) for result in batch], 
                                         batch_first=True, padding_value=pad_token_id).to(device)
        attack_prefix_ids = []
        
        # Extract attack prefixes from attack strings
        for result in batch:
            attack_tokens = tokenizer.encode(result.x_attack_str, return_tensors='pt').squeeze() # type: ignore
            if attack_tokens.ndim == 0:
                attack_tokens = attack_tokens.unsqueeze(0)
            attack_prefix_ids.append(attack_tokens)
        
        attack_prefix_ids = pad_sequence(attack_prefix_ids, batch_first=True, padding_value=pad_token_id).to(device)
        
        # Create attention masks
        original_attention_mask = (original_prefix_ids != pad_token_id).long()
        attack_attention_mask = (attack_prefix_ids != pad_token_id).long()
        
        # Compute perplexity for attack prefixes
        attack_perplexities = get_batch_perplexity(lightning_model, attack_prefix_ids, attack_attention_mask)
        prefix_perplexities.extend(attack_perplexities.cpu().tolist())
        
        # Compute token repetition for attack prefixes
        attack_repetitions = count_repeated_tokens(attack_prefix_ids, attack_attention_mask)
        total_token_repetitions.extend(attack_repetitions.cpu().tolist())
        
        # Compute semantic similarity and character-level metrics
        for i, result in enumerate(batch):
            # Get original and attack prefix texts
            original_text = tokenizer.decode(result.original_prefix_ids, skip_special_tokens=True)
            attack_text = result.x_attack_str
            
            # Semantic similarity
            semantic_sim = compute_semantic_similarity(attack_text, original_text, semantic_model)
            semantic_similarities.append(semantic_sim)
            
            # Non-alphanumeric ratio
            non_alpha_ratio = compute_non_alphanumeric_ratio(attack_text)
            non_alphanumeric_ratios.append(non_alpha_ratio)
            
            # Maximum consecutive token repetition
            max_consecutive = compute_max_consecutive_token_repetition(
                attack_prefix_ids[i], attack_attention_mask[i]
            )
            max_consecutive_repetitions.append(max_consecutive)
    
    # Compute aggregate metrics
    metrics = {
        f"naturalness_{step_interval}steps/mean_prefix_perplexity": sum(prefix_perplexities) / len(prefix_perplexities),
        f"naturalness_{step_interval}steps/std_prefix_perplexity": torch.tensor(prefix_perplexities, dtype=torch.float32).std().item(),

        f"naturalness_{step_interval}steps/mean_total_token_repetition": sum(total_token_repetitions) / len(total_token_repetitions),
        f"naturalness_{step_interval}steps/std_total_token_repetition": torch.tensor(total_token_repetitions, dtype=torch.float32).std().item(),

        f"naturalness_{step_interval}steps/mean_semantic_similarity": sum(semantic_similarities) / len(semantic_similarities),
        f"naturalness_{step_interval}steps/std_semantic_similarity": torch.tensor(semantic_similarities, dtype=torch.float32).std().item(),

        f"naturalness_{step_interval}steps/mean_non_alphanumeric_ratio": sum(non_alphanumeric_ratios) / len(non_alphanumeric_ratios),
        f"naturalness_{step_interval}steps/std_non_alphanumeric_ratio": torch.tensor(non_alphanumeric_ratios, dtype=torch.float32).std().item(),

        f"naturalness_{step_interval}steps/mean_max_consecutive_repetition": sum(max_consecutive_repetitions) / len(max_consecutive_repetitions),
        f"naturalness_{step_interval}steps/std_max_consecutive_repetition": torch.tensor(max_consecutive_repetitions, dtype=torch.float32).std().item(),

        f"naturalness_{step_interval}steps/num_samples": len(gcg_results),
    }
    
    return metrics

